{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../scripts/\")\n",
    "\n",
    "from dataset import *\n",
    "from evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T02:00:43.891802Z",
     "start_time": "2020-08-29T02:00:43.887237Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T02:00:43.891802Z",
     "start_time": "2020-08-29T02:00:43.887237Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_DATA = Path.home()/Path('code/microsoft-recommendation-contest/data/')\n",
    "sys.path.append(str(PATH_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHaS = 10 # epoch\n",
    "LR = 5e-3  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "A few main modules.\n",
    "\n",
    "- Load data\n",
    "- Construct label dataset, containing two parts per sample\n",
    "    - input: [],[] list of user historical news, list of news in impression\n",
    "    - ouput: [] list of labels indicating which news are clicked (0/1) in the impression\n",
    "- Model\n",
    "    - Train (in word2vec approach, no need)\n",
    "        - input: dataset\n",
    "        - ouput: trained model, evaluation per epoch(optional), tensorboard data(optional)\n",
    "    - Evaluate\n",
    "        - input: dataset\n",
    "        - ouput: all 4 scores\n",
    "    - Inference\n",
    "        - input: dataset\n",
    "        - ouput: ranked recommendation result as of required format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/MINDsmall_train/behaviors.tsv, data shape: (156965, 5)\n",
      "../data/MINDsmall_train/news.tsv, data shape: (51282, 8)\n",
      "../data/MINDsmall_dev/behaviors.tsv, data shape: (73152, 5)\n",
      "../data/MINDsmall_dev/news.tsv, data shape: (42416, 8)\n"
     ]
    }
   ],
   "source": [
    "behaviors_train, news_train = load_data('MINDsmall_train')\n",
    "behaviors_val, news_val = load_data('MINDsmall_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T02:26:46.059073Z",
     "start_time": "2020-08-29T02:26:46.030733Z"
    }
   },
   "outputs": [],
   "source": [
    "# sel columns \n",
    "news_col_sel = ['news_id','title','abstract']\n",
    "behaviors_col_sel = ['impression_id','user_id','history','impressions']\n",
    "\n",
    "news_train = news_train[news_col_sel]\n",
    "news_val = news_val[news_col_sel]\n",
    "behaviors_train = behaviors_train[behaviors_col_sel]\n",
    "behaviors_val = behaviors_val[behaviors_col_sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T02:26:51.586035Z",
     "start_time": "2020-08-29T02:26:51.567475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112095</th>\n",
       "      <td>112096</td>\n",
       "      <td>U23964</td>\n",
       "      <td>N51892 N37327 N2203 N42458 N9933 N54496 N59649...</td>\n",
       "      <td>N56193-1 N27581-0 N37870-0 N12042-0 N18870-0 N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        impression_id user_id  \\\n",
       "112095         112096  U23964   \n",
       "\n",
       "                                                  history  \\\n",
       "112095  N51892 N37327 N2203 N42458 N9933 N54496 N59649...   \n",
       "\n",
       "                                              impressions  \n",
       "112095  N56193-1 N27581-0 N37870-0 N12042-0 N18870-0 N...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_train.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T02:33:03.206423Z",
     "start_time": "2020-08-29T02:33:03.192328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15733</th>\n",
       "      <td>N58470</td>\n",
       "      <td>Law to allow speed limit increase on Oklahoma ...</td>\n",
       "      <td>A law that will allow the speed limit to be in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_id                                              title  \\\n",
       "15733  N58470  Law to allow speed limit increase on Oklahoma ...   \n",
       "\n",
       "                                                abstract  \n",
       "15733  A law that will allow the speed limit to be in...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_train.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65238, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.concat([news_train,news_val]).drop_duplicates()\n",
    "news_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate News Embedding\n",
    "\n",
    "Embedding can be created in various ways.\n",
    "1. pooling of word embedding <- use this one in this notebook\n",
    "2. news title+abstract embedding based on language model\n",
    "3. end-to-end trained from network\n",
    "4. from graph relation\n",
    "5. entity embedding provided\n",
    "6. mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_file = '../data/glove.6B/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, word_ind  = load_w2v_from_file(glove_embedding_file, dim_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N55528</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N19639</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N61837</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  news_id                                              title  \\\n",
       "0  N55528  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1  N19639                      50 Worst Habits For Belly Fat   \n",
       "2  N61837  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "\n",
       "                                            abstract  \n",
       "0  Shop the notebooks, jackets, and more that the...  \n",
       "1  These seemingly harmless habits are holding yo...  \n",
       "2  Lt. Ivan Molchanets peeked over a parapet of s...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_emb_gen_from_df(news_df, embedding, word_ind):\n",
    "    \"\"\"\n",
    "    create news embedding from a dataframe\n",
    "    return a dictionary whose key is news id and value is a vector\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "      \n",
    "    for _,row in news_df.iterrows():\n",
    "        \n",
    "        vec = np.array([embedding[word_ind.get(x,0)] for x in row[TITLE].split()])\n",
    "        result[row[NEWS_ID]]=np.mean(vec, axis=0)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# function test\n",
    "# process_emb_df(df_emb, dim_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "news_2_vec = news_emb_gen_from_df(pd.concat([news_train, news_val]), embedding, word_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_news_embedding(x):\n",
    "    emb = []\n",
    "    news_2_index = dict() # index starts from 1, reserve 0 for padding\n",
    "    index_2_news = ['PADDING']\n",
    "    \n",
    "    for i, news_id in enumerate(news_2_vec):\n",
    "        news_2_index[news_id] = i+1\n",
    "        emb.append(torch.tensor(news_2_vec[news_id]))\n",
    "        index_2_news.append(news_id)\n",
    "    \n",
    "#     emb.insert(0, torch.randn(len(emb[0])))  # insert a randomized embedding for padding\n",
    "    emb.insert(0, torch.zeros(len(emb[0])))  # insert a zero vector for padding\n",
    "    weight = torch.stack(emb)\n",
    "#     embedding = nn.Embedding.from_pretrained(weight, freeze=True)\n",
    "    \n",
    "    return weight, news_2_index, index_2_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_embedding, news_2_index, index_2_news = build_news_embedding(news_2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65238"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_2_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65238"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_2_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Dataset\n",
    "\n",
    "- Construct label dataset, containing two parts per sample\n",
    "    - input: [],[] list of user historical news, list of news in impression\n",
    "    - ouput: [] list of labels indicating which news are clicked (0/1) in the impression\n",
    "\n",
    "Build Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List-wise training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BehaviorDataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, behavior_df, mode = 'train'):\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        mode: train, eval, inference\n",
    "        \"\"\"\n",
    "        self.df = behavior_df\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        r = self.df.iloc[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        history = r['history'].split()\n",
    "        if self.mode in ('train','eval'):\n",
    "            impressions = [x.split('-')[0] for x in r['impressions'].split(' ')]\n",
    "            labels = [int(x[-1]) for x in r['impressions'].split(' ')]\n",
    "        else:\n",
    "            impressions = [x for x in r['impressions'].split(' ')]\n",
    "            labels = None\n",
    "\n",
    "        return (history,impressions,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BehaviorDataset(behaviors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_listwise(batch, max_len = 20):\n",
    "    # because there is no paddng, so only support batch size =1 \n",
    "    history_list, impressions_list, label_list = [], [], []\n",
    "    for history,impressions,labels in batch:\n",
    "        if len(history)>max_len:\n",
    "            history_list.append([news_2_index.get(news_id, 0) for news_id in history[-20:]])\n",
    "        else:\n",
    "            n = len(history)\n",
    "            tmp = [0]*(max_len-n) + [news_2_index.get(news_id, 0) for news_id in history]\n",
    "            history_list.append(tmp)\n",
    "#         history_list.append([news_2_index.get(news_id, 0) for news_id in history])\n",
    "        impressions_list.append([news_2_index.get(news_id, 0) for news_id in impressions])\n",
    "        label_list.append(labels)\n",
    "        \n",
    "    history_list = torch.tensor(history_list, dtype=torch.int64)\n",
    "    impressions_list = torch.tensor(impressions_list, dtype=torch.int64)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    \n",
    "    return history_list.to(device), impressions_list.to(device), label_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=1, \n",
    "    shuffle=False, num_workers=0, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pointwise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_behavior_to_points(df, is_test=False):\n",
    "    \"\"\"df: user bahaviour df\"\"\"\n",
    "    df['impression'] = df['impressions'].apply(lambda x: [y for y in x.split()])\n",
    "    df_pred = df[['impression_id','user_id','history','impression']].explode('impression')\n",
    "    df_pred['news_id'] = df_pred['impression'].apply(lambda x: x.split('-')[0])\n",
    "    \n",
    "    if not is_test:\n",
    "        df_pred['label'] = df_pred['impression'].apply(lambda x: x.split('-')[1]).astype(np.uint8)\n",
    "    \n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointwiseDataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, pointwise_df, mode = 'train'):\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        mode: train, eval, test\n",
    "        \"\"\"\n",
    "        self.df = pointwise_df\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        r = self.df.iloc[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        history = r['history'].split()\n",
    "        impressions = r['news_id']\n",
    "        label = r['label'] if self.mode in ('train','eval') else None\n",
    "\n",
    "        return (history,impressions,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_pointwise(batch):\n",
    "    history_list, impressions_list, label_list = [], [], []\n",
    "    for history,impression,label in batch:\n",
    "        history_list.append([news_2_index.get(news_id, 0) for news_id in history])\n",
    "        impressions_list.append(news_2_index.get(impression, 0))\n",
    "        label_list.append(label)\n",
    "    \n",
    "    history_lengths = torch.tensor([len(h) for h in history_list], dtype=torch.int64) # shape: B*1, B is batchsize\n",
    "    history_list = [torch.tensor(h, dtype=torch.int64) for h in history_list] \n",
    "    history_list = pad_sequence(history_list, batch_first=True, padding_value=0) # shape: B*T, T is longest length\n",
    "#     history_list = torch.tensor(history_list, dtype=torch.int64)\n",
    "    impressions_list = torch.tensor(impressions_list, dtype=torch.int64) # shape: B*1, B is batchsize\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64) # shape: B*1, B is batchsize\n",
    "    \n",
    "    return history_list.to(device), history_lengths.to(device), impressions_list.to(device), label_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training sample shape = (5723002, 6)\n"
     ]
    }
   ],
   "source": [
    "pointwise_train = explode_behavior_to_points(behaviors_train)\n",
    "pointwise_train.dropna(inplace=True)\n",
    "print(f'training sample shape = {pointwise_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive vs negative ratio is about 1:22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5491472\n",
       "1     231530\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointwise_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PointwiseDataset(pointwise_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dl = torch.utils.data.DataLoader(\n",
    "#     train_ds, batch_size=BATCH_SIZE, \n",
    "#     shuffle=True, num_workers=0, \n",
    "#     collate_fn=collate_fn_pointwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, (history, length, impression, label) in enumerate(train_dl):\n",
    "#     print(history)\n",
    "#     print(length)\n",
    "#     print(impression)\n",
    "#     print(label)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(history, impression, label):\n",
    "    print('>'*30 )\n",
    "    \n",
    "    # user\n",
    "    print('User viewed news history')\n",
    "    k = 10 # sample size\n",
    "    if len(history)>k:\n",
    "        history_sample = random.sample(history, k=10)\n",
    "    elif len(history)==0:\n",
    "        print('\\tNo history')\n",
    "        history_sample = []\n",
    "    else:\n",
    "        history_sample = history\n",
    "    \n",
    "    tokens_user = set()\n",
    "    for news_id in history_sample:\n",
    "        title = news_df.loc[news_df['news_id']==index_2_news[news_id],'title'].values[0]\n",
    "        tokens_user.update(title.split())\n",
    "        print(f'\\t{title}')\n",
    "\n",
    "    # impression\n",
    "    title = news_df.loc[news_df['news_id']==index_2_news[impression],'title'].values[0]\n",
    "    print(f'Impression: {title}')\n",
    "    tokens_news = set(title.split())\n",
    "    \n",
    "    # print shared tokens between user and news\n",
    "    print(f'Shared tokens: {tokens_news&tokens_user}')\n",
    "    \n",
    "    # label\n",
    "    print(f\"\\nPrediction: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17677</th>\n",
       "      <td>N6218</td>\n",
       "      <td>Why ex-NFL star Kellen Winslow II finally plea...</td>\n",
       "      <td>Ex-NFL tight end Kellen Winslow II struggled b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_id                                              title  \\\n",
       "17677   N6218  Why ex-NFL star Kellen Winslow II finally plea...   \n",
       "\n",
       "                                                abstract  \n",
       "17677  Ex-NFL tight end Kellen Winslow II struggled b...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.loc[news_df['news_id']==index_2_news[17678]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-335e09497556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvisualize_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dl' is not defined"
     ]
    }
   ],
   "source": [
    "for idx, (history, impression, label) in enumerate(train_dl):\n",
    "    if label.numpy()[0]==0:\n",
    "        continue\n",
    "    h, i, l = history.numpy()[0].tolist(), impression.numpy()[0], label.numpy()[0]\n",
    "    visualize_prediction(h,i,l)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on observation, there is barely any words in common between the user history and the news to predict."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "User viewed news history\n",
    "\tElizabeth Warren to Bill Gates: I'll explain my wealth tax to you\n",
    "\t'Slender Man' survivor's brother: 'I couldn't believe... that kind of thing' happened\n",
    "\tMeghan McCain confronts Trump Jr.: 'You and your family have hurt a lot of people'\n",
    "\tMid-Engine C8 Corvette Production Postponed Until February 2020\n",
    "\tFord's Big Nugget is a tiny home built inside a Transit cargo van   see inside the 4-'room' vehicle\n",
    "\tWhat Tom Brady, Lamar Jackson Told Each Other After Patriots-Ravens\n",
    "\tVideo Details Brazen Attack by Mexican Cartel on Government Forces\n",
    "\tThese Are The World's Most Magical Christmas Villages to Visit This Year\n",
    "\tWoman who made it on Delta flight without a ticket or boarding pass says 'it's not my fault'\n",
    "\tPregnant U.S. women's soccer star Alex Morgan still plans to play in 2020 Summer Olympics\n",
    "Impression: This Italian Beach is Going to Start Charging Admission\n",
    "Shared tokens: {'is', 'to', 'This'}\n",
    "\n",
    "Prediction: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PointwiseModel(nn.Module):\n",
    "    def __init__(self, embedding_weight):\n",
    "        super(PointwiseModel, self).__init__()\n",
    "#         self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weight, freeze=True)\n",
    "#         self.lstm = nn.LSTM()\n",
    "#         num_class = 2\n",
    "        embed_dim = embedding_weight.shape[1] # todo: dynamically assign based on embedding size.\n",
    "        self.fc = nn.Linear(embed_dim*2, 1)\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         initrange = 0.5\n",
    "#         self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, history, length, impression):\n",
    "        # B: batch size: T: sequence length, D: embedding dimension\n",
    "        \n",
    "        # history vector\n",
    "        history_embedding = self.embedding(history)\n",
    "        # to get average of vector while skipping the padding, sum first, then divided by length\n",
    "        history_embedding = torch.sum(history_embedding, dim=1, keepdim=False) # # expect dimension: B*D\n",
    "        user_embedding = history_embedding/length.view(-1,1)  # expect dimension: B*D\n",
    "        \n",
    "        # impression vector\n",
    "        impressions_embedding = self.embedding(impression)  # expect dimension: B*D\n",
    "#         similarities = F.cosine_similarity(user_embedding, impressions_embedding)\n",
    "\n",
    "        h = torch.cat((impressions_embedding, user_embedding), dim=1)\n",
    "        log_probs = F.log_softmax(self.fc(h), dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (history, length, impression, label) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(history, length, impression)\n",
    "#         loss = criterion(predicted_label, label.type(torch.cuda.FloatTensor)) # MSE expect float tensor, not Long \n",
    "        loss = criterion(predicted_label, label.type(torch.cuda.FloatTensor).unsqueeze(1)) # MSE expect float tensor, not Long \n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += ((predicted_label.squeeze()>THRESHOLD) == label).sum()#.item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | accuracy {:8.3f}'.format(\n",
    "                epoch, idx, len(dataloader),total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "#             print(f'debug: {predicted_label.size()}')\n",
    "#             print(f'debug: {label.size()}')\n",
    "#             print(f'debug:{((predicted_label.squeeze()>THRESHOLD) == label).sum()}')\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (history, length, impression, label) in enumerate(dataloader):\n",
    "            predicted_label = model(history, length, impression)\n",
    "            loss = criterion(predicted_label, label.type(torch.cuda.FloatTensor))\n",
    "            total_acc += ((predicted_label>THRESHOLD) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointwiseModel(\n",
       "  (embedding): Embedding(65239, 50)\n",
       "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PointwiseModel(embedding_weight=news_embedding)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5e-2  # learning rate\n",
    "BATCH_SIZE = 1024 # batch size for training\n",
    "THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "total_accu = None\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_fn_pointwise)\n",
    "valid_dataloader = torch.utils.data.DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_fn_pointwise)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "#                              shuffle=True, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 5310 batches | accuracy    0.960\n",
      "| epoch   1 |  1000/ 5310 batches | accuracy    0.959\n",
      "| epoch   1 |  1500/ 5310 batches | accuracy    0.960\n",
      "| epoch   1 |  2000/ 5310 batches | accuracy    0.960\n",
      "| epoch   1 |  2500/ 5310 batches | accuracy    0.959\n",
      "| epoch   1 |  3000/ 5310 batches | accuracy    0.959\n",
      "| epoch   1 |  3500/ 5310 batches | accuracy    0.959\n",
      "| epoch   1 |  4000/ 5310 batches | accuracy    0.960\n",
      "| epoch   1 |  4500/ 5310 batches | accuracy    0.959\n",
      "| epoch   1 |  5000/ 5310 batches | accuracy    0.960\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([1024])) must be the same as input size (torch.Size([1024, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-62d310555270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0maccu_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_accu\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtotal_accu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0maccu_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-0e3df67c3f20>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mtotal_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_label\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mTHRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtotal_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    630\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                                                   reduction=self.reduction)\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2580\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([1024])) must be the same as input size (torch.Size([1024, 1]))"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate user embedding\n",
    "\n",
    "Embedding can be created in various ways.\n",
    "1. pooling of news embedding\n",
    "3. end-to-end trained model\n",
    "4. from graph relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_emb_gen_from_pooling(behavior_df, news_2_vec):\n",
    "    \"\"\"\n",
    "    create user embedding by the pooling of its corresponding news embedding\n",
    "    return a dictionary whose key is user id and value is a vector\n",
    "    \"\"\"\n",
    "    behavior_df['history'].fillna('',inplace=True)\n",
    "    \n",
    "    result = {}\n",
    "    for _,row in behavior_df.iterrows():\n",
    "        if len(row['history'])>0:\n",
    "            vec = np.array([news_2_vec.get(news_id) for news_id in row['history'].split()] )\n",
    "            vec = np.mean(vec,axis=0)\n",
    "            result[row[USER_ID]] = vec\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "user_2_vec = user_emb_gen_from_pooling(pd.concat([behaviors_train, behaviors_val]), news_2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def user_emb_gen_from_pooling(behavior_df, news_2_vec):\n",
    "#     \"\"\"\n",
    "#     create user embedding by the pooling of its corresponding news embedding\n",
    "#     return nn.Embedding instance and word_to_index dictionary\n",
    "#     \"\"\"\n",
    "    \n",
    "#     vocab = []\n",
    "#     emb = []\n",
    "    \n",
    "#     for _,row in behavior_df.iterrows():\n",
    "#         lookup_list = [news_2_vec.get(news_id) for news_id in row['history'].split()]\n",
    "#         lookup_tensor = torch.tensor(lookup_list, dtype=torch.long)\n",
    "#         vec = torch.mean(news_emb(lookup_tensor),axis=0).squeeze()\n",
    "#         print(vec.size())\n",
    "        \n",
    "#         vocab.append(row['user_id'])\n",
    "#         emb.append(vec)\n",
    "    \n",
    "#     print(emb)\n",
    "#     emb.insert(0, torch.randn(len(vec)))  # insert a randomized embedding for padding\n",
    "#     weight = torch.stack(emb)\n",
    "#     embedding = nn.Embedding.from_pretrained(weight, freeze=True)\n",
    "    \n",
    "#     word_ind = {w:i+1 for i,w in enumerate(vocab)}  # index starts from 1, reserve 0 for padding\n",
    "    \n",
    "#     return embedding, word_ind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test function\n",
    "# user_emb, user_word_index = user_emb_gen_from_pooling(None, news_emb, news_word_index, behaviors_train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(user_v,news_v):\n",
    "    return np.dot(user_v, news_v)/(np.linalg.norm(user_v)*np.linalg.norm(news_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_from_behavior(df):\n",
    "#     df['is_clicked'] =\n",
    "    return df['impressions'].apply(lambda s: [int(x[-1]) for x in s.split(' ')])\n",
    "\n",
    "\n",
    "def predict_from_behavior(df):\n",
    "    def _calculate_relevence(r):\n",
    "        news = [x.split('-')[0] for x in r['impressions'].split(' ')]\n",
    "        global news_2_vec, user_2_vec\n",
    "        \n",
    "        # if user id is not calculated, pad 0 as relevence\n",
    "        if r['user_id'] not in user_2_vec:\n",
    "            return np.zeros(len(news))\n",
    "        \n",
    "        # construct similarity\n",
    "        user_v = user_2_vec.get(r['user_id'])\n",
    "        relevence = [cosine_sim(news_2_vec.get(news_id),user_v) if news_id in news_2_vec else 0 for news_id in news ]\n",
    "        \n",
    "        rank = np.argsort(np.argsort(relevence)[::-1]) + 1  # really trick i would say. check https://github.com/numpy/numpy/issues/8757\n",
    "        return 1./rank\n",
    "    return df.progress_apply(_calculate_relevence, axis=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "behaviors_train['is_clicked_pred'] = predict_from_behavior(behaviors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_train['is_clicked'] = get_label_from_behavior(behaviors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_from_relevence(behaviors_train['is_clicked'], behaviors_train['is_clicked_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty user embedding is only a small portion\n",
    "behaviors_train[behaviors_train['is_clicked_pred'].apply(sum)==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "behaviors_val['is_clicked_pred'] = predict_from_behavior(behaviors_val)\n",
    "behaviors_val['is_clicked'] = get_label_from_behavior(behaviors_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_auc = behaviors_val.progress_apply(lambda r: roc_auc_score(r['is_clicked'],r['is_clicked_pred']), axis=1).mean()\n",
    "print(group_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pred_dataset(df, is_test=False):\n",
    "    \"\"\"df: user bahaviour df\"\"\"\n",
    "    df['impression'] = df['impressions'].apply(lambda x: [y for y in x.split()])\n",
    "    df_pred = df[['user_id','impression']].explode('impression')\n",
    "    df_pred['news_id'] = df_pred['impression'].apply(lambda x: x.split('-')[0])\n",
    "    \n",
    "    if not is_test:\n",
    "        df_pred['label'] = df_pred['impression'].apply(lambda x: x.split('-')[1]).astype(np.uint8)\n",
    "    \n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = generate_pred_dataset(behaviors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(r):\n",
    "    user_v = user_2_vec.get(r[USER_ID], None)\n",
    "    news_v = news_2_vec.get(r[NEWS_ID], None)\n",
    "    \n",
    "    if user_v is not None and news_v is not None:\n",
    "        return np.dot(user_v, news_v)/(np.linalg.norm(user_v)*np.linalg.norm(news_v))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = df2.progress_apply(evaluate,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred.values+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_pred.sample(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(df2[LABEL].values,(y_pred.values+1)/2,pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, color='b', lw=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is no better than random guess. Need to validate the news vector. To see if closet vectors are making sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_click(impressions):\n",
    "    return sum([int(x[-1]) for x in impressions.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_val['impressions'].apply(count_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pred_net, user_emb, news_emb, pred_dataset):\n",
    "    \"\"\"\n",
    "    generate a score between (0-1) on whether this news should be recommmended to this user\n",
    "    \n",
    "    pred_dataset: two values per instance, user_id and news_id\n",
    "    \"\"\"\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
